{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Performance Metrics in Classifications\n",
    "\n",
    "### Chanil Park\n",
    "\n",
    "## Classification methods\n",
    "- kNN\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "- Linear Discriminant Analysis\n",
    "- Multi-layer Perceptron\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic commands, sets the backend of matplotlib to the 'inline' backend\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#from utilities.losses import compute_loss\n",
    "#from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize seed\n",
    "seed = 1000\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "test_size = 0.3\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.9  # step size\n",
    "max_iters = 100  # max iterations\n",
    "tol = 0.1 #SGDRegressor, Ridge\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # File Path is dependent on the starting directory path of Jupyter.\n",
    "    # df = pd.read_csv(\"../data/adult.csv\")\n",
    "    train_data = pd.read_csv(\"../Part 2 Performance Metrics in Classification/data/adult.csv\")\n",
    "    test_data = pd.read_csv(\"../Part 2 Performance Metrics in Classification/data/adultTest.csv\")\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDummies(data):\n",
    "    # Create dummy variables for categorical data\n",
    "    cat_vars = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "    for var in cat_vars:\n",
    "        cat_list = 'var' + '_' + var\n",
    "        cat_list = pd.get_dummies(data[var], prefix=var)\n",
    "        datal=data.join(cat_list)\n",
    "        data=datal\n",
    "    # Determine the categorical columns\n",
    "    data_vars = data.columns.values.tolist()\n",
    "    to_keep=[i for i in data_vars if i not in cat_vars]\n",
    "    # Removes categorical columns\n",
    "    data_final = data[to_keep]\n",
    "    data_final.columns.values\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanEducation(data):\n",
    "    # simplify the categories\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '1st-4th', 'basic', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '5th-6th', 'basic', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '7th-8th', 'basic', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '9th', 'basic', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '10th', 'basic', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '11th', 'HighSchool', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == '12th', 'HighSchool', data[\"education\"])\n",
    "    data[\"education\"]=np.where(data[\"education\"] == 'HS-grad', 'HighSchool', data[\"education\"])\n",
    "    data[\"education\"].unique()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Standardize Data Set\n",
    "    \"\"\"\n",
    "    #train_mean = train_data.mean()\n",
    "    #train_std = train_data.std()\n",
    "    #train_data = (train_data - train_mean) / train_std\n",
    "    #test_data = (test_data - train_mean) / train_std\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPreparation(data):\n",
    "    \"\"\"\n",
    "    First preperation, drop the class and prepare data\n",
    "    \"\"\"\n",
    "    data_full = data.copy()\n",
    "    data = data.drop([\"class\"], axis = 1)\n",
    "    labels = data_full[\"class\"]\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    return data_full, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(train_data, test_data):\n",
    "    # drop class and prepare data(both train and test)\n",
    "    train_data_full, train_data, train_labels = firstPreparation(train_data)\n",
    "    test_data_full, test_data, test_labels = firstPreparation(test_data)\n",
    "    # resize training data\n",
    "    #train_data = pd.DataFrame(train_data.values.reshape(test_data.shape))\n",
    "    train_data = cleanEducation(train_data)\n",
    "    test_data = cleanEducation(test_data)\n",
    "    # Handling categorized data\n",
    "    train_data = createDummies(train_data)\n",
    "    test_data = createDummies(test_data)\n",
    "    #train_data = train_data.drop([\"native-country_Holand-Netherlands\"], axis = 1)\n",
    "    # Standardize the inputs\n",
    "    train_data, test_data = standardize(train_data, test_data)\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, thetas):\n",
    "    return x.dot(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels):\n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    # prediction = cross_val_predict(classifier, train_data, test_data, cv=10)\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    # print(\"Classifier fit is done\")\n",
    "    prediction = classifier.predict(test_data)\n",
    "    # print(\"Prediction is done\")\n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = round((end_time - start_time).total_seconds(), 2)  # Track execution time\n",
    "    accuracy = round(accuracy_score(test_labels, prediction), 2)\n",
    "    precision = round(precision_score(test_labels, prediction), 2)\n",
    "    recall = round(recall_score(test_labels, prediction), 2)\n",
    "    f1 = round(f1_score(test_labels, prediction), 2)\n",
    "    print(classifier)\n",
    "    print(\"Exection time: \", exection_time)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)  # R2 should be maximize\n",
    "    print(\"F1: \", f1)\n",
    "    print(classification_report(test_labels, prediction))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check meaning of each columns\n",
    "# data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether there is missing values\n",
    "# data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at data\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at kind of values\n",
    "#for column in data.columns:\n",
    "#    print(column, \":\" , data[column].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the categories of education\n",
    "# data[\"education\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand more about the data\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distributions of each columns (numerics)\n",
    "# data.hist(bins=10,figsize=(14,10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look deeper into data by class\n",
    "# data.groupby(\"class\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look ata data by occupation\n",
    "# data.groupby(\"occupation\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look ata data by race\n",
    "# data.groupby(\"race\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passing\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Exection time:  40.08\n",
      "Accuracy:  0.82\n",
      "Precision:  0.63\n",
      "Recall:  0.55\n",
      "F1:  0.59\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.90      0.88     12435\n",
      "          1       0.63      0.55      0.59      3846\n",
      "\n",
      "avg / total       0.81      0.82      0.81     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "GaussianNB(priors=None)\n",
      "Exection time:  0.1\n",
      "Accuracy:  0.36\n",
      "Precision:  0.27\n",
      "Recall:  0.98\n",
      "F1:  0.42\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.17      0.28     12435\n",
      "          1       0.27      0.98      0.42      3846\n",
      "\n",
      "avg / total       0.79      0.36      0.32     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Exection time:  31.23\n",
      "Accuracy:  0.85\n",
      "Precision:  0.75\n",
      "Recall:  0.54\n",
      "F1:  0.63\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.94      0.90     12435\n",
      "          1       0.75      0.54      0.63      3846\n",
      "\n",
      "avg / total       0.84      0.85      0.84     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Exection time:  0.19\n",
      "Accuracy:  0.81\n",
      "Precision:  0.59\n",
      "Recall:  0.61\n",
      "F1:  0.6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.87      0.88     12435\n",
      "          1       0.59      0.61      0.60      3846\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Exection time:  0.31\n",
      "Accuracy:  0.84\n",
      "Precision:  0.7\n",
      "Recall:  0.56\n",
      "F1:  0.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.93      0.90     12435\n",
      "          1       0.70      0.56      0.62      3846\n",
      "\n",
      "avg / total       0.83      0.84      0.83     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "Exection time:  1.08\n",
      "Accuracy:  0.86\n",
      "Precision:  0.77\n",
      "Recall:  0.6\n",
      "F1:  0.68\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.94      0.91     12435\n",
      "          1       0.77      0.60      0.68      3846\n",
      "\n",
      "avg / total       0.86      0.86      0.86     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Exection time:  4.31\n",
      "Accuracy:  0.87\n",
      "Precision:  0.8\n",
      "Recall:  0.6\n",
      "F1:  0.69\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.95      0.92     12435\n",
      "          1       0.80      0.60      0.69      3846\n",
      "\n",
      "avg / total       0.87      0.87      0.86     16281\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\ncksd\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier fit is done\n",
      "Prediction is done\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)\n",
      "Exection time:  0.22\n",
      "Accuracy:  0.84\n",
      "Precision:  0.72\n",
      "Recall:  0.56\n",
      "F1:  0.63\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.93      0.90     12435\n",
      "          1       0.72      0.56      0.63      3846\n",
      "\n",
      "avg / total       0.83      0.84      0.84     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "Exection time:  20.53\n",
      "Accuracy:  0.84\n",
      "Precision:  0.68\n",
      "Recall:  0.56\n",
      "F1:  0.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.92      0.90     12435\n",
      "          1       0.68      0.56      0.62      3846\n",
      "\n",
      "avg / total       0.83      0.84      0.83     16281\n",
      "\n",
      "\n",
      "\n",
      "Classifier fit is done\n",
      "Prediction is done\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Exection time:  1.03\n",
      "Accuracy:  0.85\n",
      "Precision:  0.73\n",
      "Recall:  0.59\n",
      "F1:  0.65\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.93      0.91     12435\n",
      "          1       0.73      0.59      0.65      3846\n",
      "\n",
      "avg / total       0.85      0.85      0.85     16281\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_data, test_data = load_data();\n",
    "# Preprocess the data\n",
    "train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(train_data, test_data)\n",
    "classifier = KNeighborsClassifier()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = GaussianNB()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = SVC()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = DecisionTreeClassifier()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = RandomForestClassifier()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = AdaBoostClassifier()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = GradientBoostingClassifier()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = MLPClassifier()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "classifier = LogisticRegression()\n",
    "applyClassifierThenResult(classifier, train_data, train_labels, test_data_full, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
