{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Optimisation Methods\n",
    "\n",
    "### Chanil Park\n",
    "\n",
    "## regression methods\n",
    "- linear regression\n",
    "\n",
    "## Optimisation methods\n",
    "- Stochastic Gradient Descent\n",
    "- Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from utilities.losses import compute_loss\n",
    "from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# General settings\n",
    "from utilities.visualization import visualize_train, visualize_test\n",
    "\n",
    "# Initialize seed\n",
    "seed = 0\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.3\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.1  # step size\n",
    "max_iters = 50  # max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Data from CSV\n",
    "    :return: df    a panda data frame\n",
    "    \"\"\"\n",
    "    # df = pd.read_csv(\"../data/Part2.csv\")\n",
    "    df = pd.read_csv(\"../Part 3 Optimisation Methods/data/Part2.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # Split the data into train and test\n",
    "    train_data, test_data = train_test_split(data, test_size = train_test_split_test_size)\n",
    "\n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full = train_data.copy()\n",
    "    train_data = train_data.drop([\"Height\"], axis = 1)\n",
    "    train_labels = train_data_full[\"Height\"]\n",
    "\n",
    "    test_data_full = test_data.copy()\n",
    "    test_data = test_data.drop([\"Height\"], axis = 1)\n",
    "    test_labels = test_data_full[\"Height\"]\n",
    "\n",
    "    # Standardize the inputs\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "\n",
    "    # Tricks: add dummy intercept to both train and test\n",
    "    train_data['intercept_dummy'] = pd.Series(1.0, index = train_data.index)\n",
    "    test_data['intercept_dummy'] = pd.Series(1.0, index = test_data.index)\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(y, x, theta, max_iters, alpha, optimizer_type = \"BGD\", metric_type = \"MSE\"):\n",
    "    \"\"\"\n",
    "    Learn to estimate the regression parameters (i.e., w and b)\n",
    "    :param y:                   train labels\n",
    "    :param x:                   train data\n",
    "    :param theta:               model parameter\n",
    "    :param max_iters:           max training iterations\n",
    "    :param alpha:               step size\n",
    "    :param optimizer_type:      optimizer type (default: Batch Gradient Descient): GD, SGD, MiniBGD or PSO\n",
    "    :param metric_type:         metric type (MSE, RMSE, R2, MAE). NOTE: MAE can't be optimized by GD methods.\n",
    "    :return: thetas              all updated model parameters tracked during the learning course\n",
    "             losses             all losses tracked during the learning course\n",
    "    \"\"\"\n",
    "    thetas = None\n",
    "    losses = None\n",
    "    if optimizer_type == \"BGD\":\n",
    "        thetas, losses = gradient_descent(y, x, theta, max_iters, alpha, metric_type)\n",
    "    elif optimizer_type == \"MiniBGD\":\n",
    "        thetas, losses = mini_batch_gradient_descent(y, x, theta, max_iters, alpha, metric_type, mini_batch_size = 10)\n",
    "    elif optimizer_type == \"PSO\":\n",
    "        thetas, losses = pso(y, x, theta, max_iters, 100, metric_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"[ERROR] The optimizer '{ot}' is not defined, please double check and re-run your program.\".format(\n",
    "                ot = optimizer_type))\n",
    "    return thetas, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn: execution time=0.000 seconds\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-23b8563cd3e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Build baseline model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"R2:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"R2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# R2 should be maximize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSE:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MSE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RMSE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Settings\n",
    "    metric_type = \"MSE\"  # MSE, RMSE, MAE, R2\n",
    "    optimizer_type = \"MiniBGD\"  # PSO, BGD, MiniBGD\n",
    "\n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "\n",
    "    # Step 3: Learning Start\n",
    "    theta = np.array([0.0, 0.0])  # Initialize model parameter\n",
    "\n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    thetas, losses = learn(train_labels.values, train_data.values, theta, max_iters, alpha, optimizer_type, metric_type)\n",
    "\n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = (end_time - start_time).total_seconds()  # Track execution time\n",
    "\n",
    "    # Step 4: Results presentation\n",
    "    print(\"Learn: execution time={t:.3f} seconds\".format(t = exection_time))\n",
    "\n",
    "    # Build baseline model\n",
    "    print(\"R2:\", -compute_loss(test_labels.values, test_data.values, thetas[-1], \"R2\"))  # R2 should be maximize\n",
    "    print(\"MSE:\", compute_loss(test_labels.values, test_data.values, thetas[-1], \"MSE\"))\n",
    "    print(\"RMSE:\", compute_loss(test_labels.values, test_data.values, thetas[-1], \"RMSE\"))\n",
    "    print(\"MAE:\", compute_loss(test_labels.values, test_data.values, thetas[-1], \"MAE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
