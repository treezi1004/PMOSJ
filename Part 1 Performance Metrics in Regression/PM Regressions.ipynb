{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Performance Metrics in Regression\n",
    "\n",
    "### Chanil Park\n",
    "\n",
    "## regression methods\n",
    "- linear regression\n",
    "- k-neighbors regression\n",
    "- Ridge regression\n",
    "- decision tree regression\n",
    "- random forest regression\n",
    "- gradient Boosting regression\n",
    "- SGD regression\n",
    "- support vector regression (SVR)\n",
    "- linear SVR\n",
    "- multi-layer perceptron regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic commands, sets the backend of matplotlib to the 'inline' backend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Performance Metrics in Regression\n",
    "x = weight and y = height.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#from utilities.losses import compute_loss\n",
    "#from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# General settings\n",
    "#from utilities.visualization import visualize_train, visualize_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seed\n",
    "seed = 1000\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.3\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.9  # step size\n",
    "max_iters = 100  # max iterations\n",
    "tol = 0.1 #SGDRegressor, Ridge\n",
    "verbose = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Data from CSV\n",
    "    :return: df    a panda data frame\n",
    "    \"\"\"\n",
    "    # File Path is dependent on the starting directory path of Jupyter.\n",
    "    # df = pd.read_csv(\"../data/diamonds.csv\")\n",
    "    df = pd.read_csv(\"../Part 1 Performance Metrics in Regression/data/diamonds.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Standardize Data Set\n",
    "    \"\"\"\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "    return train_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPreparation(data):\n",
    "    \"\"\"\n",
    "    First preperation, drop the class and prepare data\n",
    "    \"\"\"\n",
    "    data_full = data.copy()\n",
    "    data = data.drop([\"price\"], axis = 1)\n",
    "    labels = data_full[\"price\"]\n",
    "    return data_full, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # Split the data into train and test\n",
    "    train_data, test_data = train_test_split(data, test_size = train_test_split_test_size, random_state=seed)\n",
    "\n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full, train_data, train_labels = firstPreparation(train_data)\n",
    "    test_data_full, test_data, test_labels = firstPreparation(test_data)\n",
    "    \n",
    "    # Handling categorized data\n",
    "    train_data = pd.get_dummies(train_data, columns=['cut', 'color', 'clarity'])\n",
    "    # print(train_data.head())\n",
    "    test_data = pd.get_dummies(test_data, columns=['cut', 'color', 'clarity'])    \n",
    "    # print(test_data.head())\n",
    "    \n",
    "    # Standardize the inputs\n",
    "    train_data, test_data = standardize(train_data, test_data)\n",
    "\n",
    "    # Tricks: add dummy intercept to both train and test\n",
    "    train_data['intercept_dummy'] = pd.Series(1.0, index = train_data.index)\n",
    "    test_data['intercept_dummy'] = pd.Series(1.0, index = test_data.index)\n",
    "    # print(train_data.head())\n",
    "    # print(test_data.head())    \n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, thetas):\n",
    "    return x.dot(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels):\n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    model.fit(train_data, train_labels)\n",
    "    prediction = model.predict(test_data)\n",
    "    #pred_label = predict(test_data, gradient_ws[-1])\n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = round((end_time - start_time).total_seconds(), 2)  # Track execution time\n",
    "    mse = round(mean_squared_error(test_labels, prediction), 2)\n",
    "    rmse = round(np.sqrt(mse), 2)\n",
    "    r2_error = round(r2_score(test_labels, prediction), 2)\n",
    "    mae = round(mean_absolute_error(test_labels, prediction), 2)\n",
    "    print(model)\n",
    "    print(\"Exection time: \", exection_time)\n",
    "    #print(\"Coefficients: \", model.coef_)\n",
    "    #print(\"Intercept: \", model.intercept_)\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"R2: \", r2_error)  # R2 should be maximize\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"\\n\")\n",
    "    #plt.scatter(x=test_data_full[\"carat\"], y=test_data_full[\"price\"], color='blue')\n",
    "    #plt.plot(test_data_full[\"carat\"], pred_label, color='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "Exection time:  0.05\n",
      "MSE:  1302827.0\n",
      "RMSE:  1141.41\n",
      "R2:  0.92\n",
      "MAE:  746.96\n",
      "\n",
      "\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Exection time:  9.55\n",
      "MSE:  751250.25\n",
      "RMSE:  866.75\n",
      "R2:  0.95\n",
      "MAE:  399.62\n",
      "\n",
      "\n",
      "Ridge(alpha=0.9, copy_X=True, fit_intercept=True, max_iter=100,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.1)\n",
      "Exection time:  0.02\n",
      "MSE:  1302617.9\n",
      "RMSE:  1141.32\n",
      "R2:  0.92\n",
      "MAE:  747.44\n",
      "\n",
      "\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best')\n",
      "Exection time:  0.37\n",
      "MSE:  800.72\n",
      "RMSE:  28.3\n",
      "R2:  1.0\n",
      "MAE:  2.67\n",
      "\n",
      "\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Exection time:  2.0\n",
      "MSE:  766.96\n",
      "RMSE:  27.69\n",
      "R2:  1.0\n",
      "MAE:  3.33\n",
      "\n",
      "\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "Exection time:  3.55\n",
      "MSE:  55149.58\n",
      "RMSE:  234.84\n",
      "R2:  1.0\n",
      "MAE:  139.17\n",
      "\n",
      "\n",
      "SGDRegressor(alpha=0.9, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=100, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=0.0001,\n",
      "       verbose=0, warm_start=False)\n",
      "Exection time:  0.07\n",
      "MSE:  3060687.87\n",
      "RMSE:  1749.48\n",
      "R2:  0.81\n",
      "MAE:  1116.39\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\ncksd\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=100, shrinking=True, tol=0.1, verbose=False)\n",
      "Exection time:  0.68\n",
      "MSE:  45123282.17\n",
      "RMSE:  6717.39\n",
      "R2:  -1.81\n",
      "MAE:  6235.42\n",
      "\n",
      "\n",
      "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=100,\n",
      "     random_state=None, tol=0.1, verbose=0)\n",
      "Exection time:  0.05\n",
      "MSE:  2260896.52\n",
      "RMSE:  1503.63\n",
      "R2:  0.86\n",
      "MAE:  793.17\n",
      "\n",
      "\n",
      "MLPRegressor(activation='relu', alpha=0.9, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.1, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "Exection time:  19.61\n",
      "MSE:  532593.91\n",
      "RMSE:  729.79\n",
      "R2:  0.97\n",
      "MAE:  423.45\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\ncksd\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # load data \n",
    "    data = load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "\n",
    "    # Build baseline model\n",
    "    model = LinearRegression()\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = KNeighborsRegressor()\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = Ridge(max_iter=max_iters, alpha=alpha, tol=tol)\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = DecisionTreeRegressor()\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = RandomForestRegressor()\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = GradientBoostingRegressor(alpha=alpha)\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = SGDRegressor(max_iter=100, alpha=alpha, tol=0.0001)\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = SVR(max_iter=max_iters, tol=tol)\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = LinearSVR(max_iter=max_iters, tol=tol)\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    model = MLPRegressor(max_iter=max_iters, alpha=alpha, tol=tol)\n",
    "    applyModelThenResult(model, train_data, train_labels, test_data_full, test_data, test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
